{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43-44\n",
    "- This is a square symmetric m   m matrix in which the diagonal el- ements are the variance of the corresponding features and the off- diagonal elements are the covariance between the features. \n",
    "- mxmの平方行列。対角線は対象の特徴の分散。それ以外は各特徴量同士の共分散。\n",
    "\n",
    "- The features are decorrelated if the covariance matrix is diagonal, that is, all off-diagonal elements in C are zeros. \n",
    "\n",
    "- 非対角線が全てゼロなら、特徴量はそれぞれ無相関である。\n",
    "\n",
    "\n",
    "- If the covariance matrix is not review of predictive modeling diagonal, the features are correlated, which makes it more difficult to understand the distribution of x because it cannot be described in terms of distributions of individual features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （謎）共分散行列が？なら、特徴量は相関している。それはxの分布を理解するのをより難しくする。なぜなら、ここの特徴の分布を表現できないから？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our next step is to find the transformation of X that produces a different n   m design matrix Z in which the covariance matrix is diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xの変換を見つける。n x mのZという行列。（この行列がどんなものかは後で調べる）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the PCA approach, we make an assumption that this matrix can be obtained by using a linear transformation\n",
    "Z = XT (2.64)\n",
    "in which T is the transformation matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCAのアプローチではZは線形変換で得ることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To construct this matrix, let us first show how the design matrix can be factorized on the basis of vectors that correspond to the directions of maximum variance in the design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- この行列を作るには、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44-45\n",
    "- The first step of factorization is to find the directions of maximum variance in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 因数分解の最初はデータの中で分散が最大の方向を見つけること"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These directions can be thought of as the main axes of the point cloud in a scatter plot, as illustrated in Figure 2.8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- これらの方向は散布図の点におけるメインの軸と考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these directions can be defined as a vector, so we start by finding an m-dimensional unit vector that satisfies the following condition:\n",
    "v1   argmax ∥Xv∥2 , ∥v∥   1 (2.65) v\n",
    "This vector corresponds to the axis with maximum variance. Next, we find a second unit vector that is orthogonal to the first one to capture the remaining variance:\n",
    "v2  argmax ∥Xv∥2, ∥v∥ 1 and v v1  0 (2.66) v\n",
    "We continue this process by requiring each vector to be orthogonal to all previous ones, and, assuming that the design matrix has rank r, we can create as many as r non-zero vectors v. By construction, each vec- tor corresponds to the axis with maximum remaining variance in the design matrix. These vectors are referred to as the principal components of the design matrix.\n",
    "Let us denote an m   r matrix assembled from column vectors v as V. As all of the unit vectors that we have constructed are orthogonal, this matrix is column-orthonormal, that is\n",
    "VT V   I (2.67) Unit vectors v capture the directions of the variance but not its magni-\n",
    "tude. Let us calculate these values separately and denote them as\n",
    "σi   ∥Xvi∥ (2.68)\n",
    " Figure 2.8: Example of principal component analysis. (a) A data set of 500 nor- mally distributed points and the corresponding principal compo- nents. Features x1 and x2 are strongly correlated. (b) A decorrelated representation obtained by using PCA. Features z1 and z2 are not correlated.\n",
    "Each principal component captures only the remaining variance, so the value of σ is biggest for the first component and then decreases, in the order\n",
    "σ1 ¥σ2 ¥...¥σr (2.69) Let us denote an r   r diagonal matrix with the σ values on the main\n",
    "diagonal as\n",
    "Σ   diagpσ1,...,σrq (2.70)\n",
    "At this point, we have an orthonormal basis of vectors V and the corresponding scaling factors Σ. To complete the factorization of the design matrix, we need a third factor that projects the design matrix onto the principal component basis or, alternatively, mixes the basis into the design matrix. By denoting this factor as U, we can define the decomposition as\n",
    "X   UΣVT (2.71) for which U can be obtained by solving equation 2.71 with respect to\n",
    "this factor:\n",
    "U   XVΣ 1 (2.72)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
